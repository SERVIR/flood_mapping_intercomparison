{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SERVIR/flood_mapping_intercomparison/blob/main/notebooks/Module_2_Scripted_Product_Access.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# In this module, we will get access to the following flood products\n",
        "\n",
        "1. DSWx-HLS\n",
        "2. DSWx-S1\n",
        "3. VFM\n",
        "4. GFM\n",
        "5. MCDWD\n",
        "6. HYDRAFloods\n",
        "7. HYDROSAR\n",
        "\n",
        "We can access the DSWx-HLS and DSWx-S1 products via the earthaccess python package. In order to run this code you will need\n",
        "\n",
        "1. A NASA Earthdata login\n",
        "  * If you do not have a NASA Earthdata login, you can register via [this link](https://urs.earthdata.nasa.gov/users/new)\n",
        "2. A Copernicus Emergency Management GFM profile\n",
        "  * If you do not have a GFM profile, you can register via [this link](https://portal.gfm.eodc.eu/register)"
      ],
      "metadata": {
        "id": "Ftyc2P-ALYMu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **IMPORTANT**\n",
        "\n",
        "To access the DSWX-HLS products and run HYDROSAR, you will need a NASA Earthdata account .\n",
        " * If you do not have one, you can register [here](https://urs.earthdata.nasa.gov/users/new) free of charge\n",
        " * If you do have a NASA Earthdata account, you can login [here](https://urs.earthdata.nasa.gov/home) to double-check that your credentialS\n",
        "\n",
        "To access the GFM product, you will need a GFM account\n",
        "\n",
        "* If you do not have a GFM account, you can register [here](https://portal.gfm.eodc.eu/register) free of charge.\n",
        "* If you do have  a GFM account, you can login [here](https://portal.gfm.eodc.eu/login?redirect=%5Bobject%20Object%5D) to double-check your credentials.\n",
        "\n",
        "### **MODIFIABLE VARIABLE ALERT**\n",
        "\n",
        "Declare some of your global variables below."
      ],
      "metadata": {
        "id": "hWKI5bDasmfR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "my_earthdata_login = \"maynard_maganini\"                                     # Replace with your earthdata login (email adress) in string format\n",
        "my_earthdata_password = \"Piccolo1998!\"                                     # Replace with your earthdata password in string format\n",
        "my_gfm_email = \"mrm0065@uah.edu\"                                           # Replace with your GFM login (email adress) in string format\n",
        "my_gfm_password = \"Reese1998!\"                                          # Replace with your GFM password in string format\n",
        "\n",
        "my_gee_folder = \"users/mickymags/aug_fmi_cambodia_20241001/\"               # Make sure this ends in a slash\n",
        "my_gee_project = \"servir-sco-assets\"\n",
        "my_Gdrive_folder = 'drive/MyDrive/Flood_Intercomparison/Case_Studies/aug/aug_cambodia_20241001/'   # Make sure this ends in a slash\n",
        "my_start_date = \"2024-10-01\"\n",
        "flood_event_desc = 'aug_cambodia_20241001'"
      ],
      "metadata": {
        "id": "Df923YtDsoqh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 0: Take care of some housekeeping matters\n",
        "\n",
        "Before we start requesting some data, we need to do a couple of things\n",
        "\n",
        "1. Create a .netrc file.\n",
        "  * A .netrc file allows us to mimic the login process we would do when we sign into a website such as NASA Earthdata to download data. Usually this would be a file that exists in our home directory, but since we are working in Google Colaboratory, we can do this programatically\n",
        "2. Install and import various packages we will use throughout the rest of the notebook\n",
        "3. Define the bounding box based on the area of interest we have in Google Earth Engine\n",
        "4. Mount Google Drive"
      ],
      "metadata": {
        "id": "mxQxhVPmve45"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 0 Part 1: Create the .netrc file"
      ],
      "metadata": {
        "id": "Aw9CIRUywiDR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "my_netrc_string = \"machine urs.earthdata.nasa.gov login \" + my_earthdata_login + \" password \" + my_earthdata_password"
      ],
      "metadata": {
        "id": "l7dkkd_Ls0bj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Credit to user jruss on stackoverflow.com for the piece of code below, see [this post](https://stackoverflow.com/questions/67153514/use-request-with-nasa-data-on-google-colaboratory)"
      ],
      "metadata": {
        "id": "51bMzpsSw1lB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup NASA\n",
        "!touch /root/.netrc\n",
        "!echo $my_netrc_string >/root/.netrc\n",
        "!chmod 0600 /root/.netrc"
      ],
      "metadata": {
        "id": "uypzP_eHEsRy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 0 Part 2: Install and Import Relevant Python Packages"
      ],
      "metadata": {
        "id": "sEQ1aWmTxFHL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install boto3                                     # Install boto3\n",
        "!pip install earthaccess                               # Install earthaccess\n",
        "!pip install ipyleaflet==0.18.2 geemap hydrafloods     # Install hydrafloods and its relevant dependencies\n",
        "!pip install hydrosar asf_tools hyp3_sdk               # Install hydrosar and its relevant dependencies\n",
        "!pip install geemap\n",
        "!pip install opensarlab_lib"
      ],
      "metadata": {
        "id": "UN5euAdVMMte"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import relevant packages below"
      ],
      "metadata": {
        "id": "dCmsxrfSy2Zq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "from osgeo import gdal\n",
        "import earthaccess\n",
        "import os\n",
        "import subprocess\n",
        "import glob\n",
        "from osgeo import gdal\n",
        "import ee\n",
        "import boto3\n",
        "from botocore.client import Config\n",
        "from botocore import UNSIGNED\n",
        "import requests\n",
        "import shutil\n",
        "import hyp3_sdk as sdk\n",
        "from hydrosar.water_map import make_water_map\n",
        "from hydrafloods import corrections\n",
        "import hydrafloods as hf\n",
        "import geemap\n",
        "import numpy as np\n",
        "import zipfile\n",
        "import opensarlab_lib as osl\n",
        "import time"
      ],
      "metadata": {
        "id": "PIlu-rHdFNSI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**IMPORTANT**\n",
        "If you get an error that \"no such package exists\" for the above cell, don't worry! We just need to install the gdal package within Google Colab.\n",
        "\n",
        "If you got this error, Uncomment and run the relevant code cell(s) below, then run the code cell above again to complete the import process\n",
        "\n",
        "If you did not get this error, proceed to step 0 part 4"
      ],
      "metadata": {
        "id": "oWFt4wIV1JUL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "################################################################################\n",
        "# INSTALL GDAL ON GOOGLE COLAB\n",
        "################################################################################\n",
        "! wget https://repo.anaconda.com/miniconda/Miniconda3-py37_4.8.2-Linux-x86_64.sh\n",
        "! chmod +x Miniconda3-py37_4.8.2-Linux-x86_64.sh\n",
        "! bash ./Miniconda3-py37_4.8.2-Linux-x86_64.sh -b -f -p /usr/local\n",
        "import sys\n",
        "sys.path.append('/usr/local/lib/python3.7/site-packages/')\n",
        "\n",
        "!conda install -c conda-forge gdal\n",
        "'''"
      ],
      "metadata": {
        "id": "neZ7xEk-1iIj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Thank you and credit to Dr. Chanin Nantasenamat and his \"Data Professor\" Youtube Channel for the above code: [https://github.com/dataprofessor](https://github.com/dataprofessor)"
      ],
      "metadata": {
        "id": "F2k3IbWc1rEf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 0 part 3: Define bounding box from AOI"
      ],
      "metadata": {
        "id": "ZU_rKfo21ooI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Authenticate and initialize earthengine\n",
        "ee.Authenticate()\n",
        "ee.Initialize(project = my_gee_project)"
      ],
      "metadata": {
        "id": "BC6YM_GW2bSB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "my_aoi = ee.FeatureCollection(my_gee_folder + \"aoi\")\n",
        "coords = my_aoi.bounds().getInfo()['coordinates']\n",
        "lon_min = coords[0][0][0]\n",
        "lat_min = coords[0][0][1]\n",
        "lon_max = coords[0][2][0]\n",
        "lat_max = coords[0][2][1]"
      ],
      "metadata": {
        "id": "JZxIG_k63LlR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Get day after"
      ],
      "metadata": {
        "id": "Ac7nZwb84OvT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "myeedate = ee.Date(my_start_date)\n",
        "my_ee_dayafter = myeedate.advance(1, 'day')"
      ],
      "metadata": {
        "id": "KM7HjjIp4QWG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mydayafter_year = str(my_ee_dayafter.get('year').getInfo())\n",
        "mydayafter_month = my_ee_dayafter.get('month').getInfo()\n",
        "\n",
        "if mydayafter_month < 10:\n",
        "  monthstr = '0' + str(mydayafter_month)\n",
        "else:\n",
        "  monthstr = str(mydayafter_month)\n",
        "\n",
        "\n",
        "mydayafter_day = my_ee_dayafter.get('day').getInfo()\n",
        "\n",
        "if mydayafter_day < 10:\n",
        "  datestr = '0' + str(mydayafter_day)\n",
        "else:\n",
        "  datestr = str(mydayafter_day)\n",
        "\n",
        "myday_after = mydayafter_year + \"-\" + monthstr + '-' + datestr"
      ],
      "metadata": {
        "id": "kCprmosU4Oc_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 0 part 4: Mount Google Drive"
      ],
      "metadata": {
        "id": "spsiJT8xMMwc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "lxCkrXfgMMcd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cd drive/MyDrive/"
      ],
      "metadata": {
        "id": "aiwDHVBt3MQE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pwd"
      ],
      "metadata": {
        "id": "sykNQlCMqSyN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gdrive_split = my_Gdrive_folder.split('/')[2:-1]\n",
        "gdrive_split\n",
        "\n",
        "for i in gdrive_split:\n",
        "  stri = str(i)\n",
        "  if os.path.isdir(stri) == True:      #if folder already exists, enter into it\n",
        "    os.chdir(stri)\n",
        "  else:                                # if folder doeesn't yet exist, create it, then enter it.\n",
        "    os.mkdir(stri)\n",
        "    os.chdir(stri)"
      ],
      "metadata": {
        "id": "z0HQ_cRG3Wif"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 1: DSWx-HLS"
      ],
      "metadata": {
        "id": "o_a6dDJrMUS-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pwd"
      ],
      "metadata": {
        "id": "4yHyN4ROyTgf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ls"
      ],
      "metadata": {
        "id": "dsOgDDCqr_X-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "auth = earthaccess.login(strategy=\"netrc\")"
      ],
      "metadata": {
        "id": "HAAArcEzFYng"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dswx_results = earthaccess.search_data(short_name=\"OPERA_L3_DSWX-HLS_V1\",cloud_hosted=True,\n",
        "                                     temporal=(my_start_date,my_start_date), bounding_box=(lon_min,lat_min,lon_max,lat_max))"
      ],
      "metadata": {
        "id": "4Tq50EC3Fc_W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "downloaded = earthaccess.download(dswx_results, local_path = 'dswx_hls')"
      ],
      "metadata": {
        "id": "4C7nU5uBFmQW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pwd"
      ],
      "metadata": {
        "id": "An_HV5dmq5Fq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ls"
      ],
      "metadata": {
        "id": "SC9tUpxoq_JE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cd dswx_hls"
      ],
      "metadata": {
        "id": "-A0r9SozrAPv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ls"
      ],
      "metadata": {
        "id": "-YOBnVZYrBXU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We want to extract only the files that end in \"B02_BWTR.tif\", as these are the binary water maps for each string. We can make a list of just these scenes using the glob method"
      ],
      "metadata": {
        "id": "rRcBpfgNAGAx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Note that the 'S2A portion of the string below may need to change depending on if the HLS image is derived from a Sentinel-2 image or a Landsat-8 image'\n",
        "dswxhls_list = glob.glob(\"OPERA_L3_DSWx-HLS*B01*.tif\")"
      ],
      "metadata": {
        "id": "z964cc0PGnMq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pwd"
      ],
      "metadata": {
        "id": "1LM1O4n7sg4-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ls"
      ],
      "metadata": {
        "id": "ZkH0o--Zs3Xp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dswxhls_list"
      ],
      "metadata": {
        "id": "Ivmvv1t8LR6b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "info = gdal.Info(dswxhls_list[0])                                    # Extract the info from the first file in the list\n",
        "projection_info_index = info.find('EPSG\",32')                        # Find the projection of the file. We know they use the UTM projection which uses the EPSG code 32XXX, where XXX relates to the specific UTM zone\n",
        "theproj = info[projection_info_index:projection_info_index + 12]     # slice the string to extract the substring containing the UTM projection\n",
        "final_proj_string = theproj[0:4] + ':' + theproj[6:11]               # reformat the string into the projection format GDAL requires for us to reproject our imagery/"
      ],
      "metadata": {
        "id": "3Zr4fioG-hce"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(final_proj_string)"
      ],
      "metadata": {
        "id": "yqPuPNVQGguv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a list of output files\n",
        "\n",
        "new_dswxhls_names = []\n",
        "num_dswxhls_scenes = len(dswxhls_list)\n",
        "for j in range(num_dswxhls_scenes):\n",
        "  if j < 9:\n",
        "    dswxhls_string = \"dswxhls_pt0\" + str(j+1) + \".tif\"\n",
        "  else:\n",
        "    dswxhls_string = \"dswxhls_pt\" + str(j+1) + \".tif\"\n",
        "\n",
        "  new_dswxhls_names.append(dswxhls_string)"
      ],
      "metadata": {
        "id": "HXG4NqxUAFF0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for k in range(num_dswxhls_scenes):\n",
        "  outfile_name = new_dswxhls_names[k]\n",
        "  infile_name = dswxhls_list[k]\n",
        "  gdal.Warp(outfile_name, infile_name, dstSRS = final_proj_string)"
      ],
      "metadata": {
        "id": "D8aDuuj8Au7Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dswxhls_warped_list = glob.glob(\"dswxhls_pt[0-9][0-9].tif\")\n",
        "\n",
        "if len(dswxhls_warped_list) > 0:\n",
        "  if len(dswxhls_warped_list) == 1:\n",
        "    print('path2')\n",
        "    os.rename(dswxhls_warped_list[0], 'merged_dswxhls_' + flood_event_desc + '.tif')\n",
        "  else:\n",
        "    print('path1')\n",
        "    #vfm_warped_list = glob.glob(\"_reproj_pt[0-9].tif\")\n",
        "    cmd_dswxhls = \"gdal_merge.py -o merged_dswxhls_\" + flood_event_desc + \".tif\"\n",
        "    subprocess.call(cmd_dswxhls.split()+dswxhls_warped_list)\n",
        "else:\n",
        "    print('path3')\n",
        "    print(\"No entries in dswxhls_warped_list. Something went wrong.\")"
      ],
      "metadata": {
        "id": "dgoQHkG8114V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Part 2: DSWx-S1**"
      ],
      "metadata": {
        "id": "9s4DgHMjJUgr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.chdir('..')                                           # Return to the parent folder"
      ],
      "metadata": {
        "id": "tZq4U2CLKYIz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dswxs1_results = earthaccess.search_data(short_name=\"OPERA_L3_DSWX-S1_V1\",cloud_hosted=True,\n",
        "                                     temporal=(my_start_date,my_start_date), bounding_box=(lon_min,lat_min,lon_max,lat_max))"
      ],
      "metadata": {
        "id": "HZJDtqXBJPNd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dswxs1_downloaded = earthaccess.download(dswxs1_results, local_path = 'dswxs1')"
      ],
      "metadata": {
        "id": "_zr43By6KTEL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.chdir('dswxs1')"
      ],
      "metadata": {
        "id": "JTCOqljzJAa8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dswxs1_list = glob.glob(\"OPERA_L3_DSWx-S1*_B02*.tif\")"
      ],
      "metadata": {
        "id": "tqgv8I8vBN4z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a list of output files\n",
        "\n",
        "new_dswxs1_names = []\n",
        "num_dswxs1_scenes = len(dswxs1_list)\n",
        "for i in range(num_dswxs1_scenes):\n",
        "  if i < 9:\n",
        "    dswxs1_string = \"dswxs1_pt0\" + str(i+1) + \".tif\"\n",
        "  else:\n",
        "    dswxs1_string = \"dswxs1_pt\" + str(i+1) + \".tif\"\n",
        "\n",
        "  infile = dswxs1_list[i]\n",
        "\n",
        "  gdal.Warp(dswxs1_string, infile, dstSRS = final_proj_string)"
      ],
      "metadata": {
        "id": "vJLObIKXK4Jk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dswxs1_warped_list = glob.glob(\"dswxs1_pt[0-9][0-9].tif\")\n",
        "\n",
        "if len(dswxs1_warped_list) > 0:\n",
        "  if len(dswxs1_warped_list) == 1:\n",
        "    print('path2')\n",
        "    os.rename(dswxs1_warped_list[0], 'merged_dswxs1_' + flood_event_desc + '.tif')\n",
        "  else:\n",
        "    print('path1')\n",
        "    cmd_dswxs1 = \"gdal_merge.py -o merged_dswxs1_\" + flood_event_desc + \".tif\"\n",
        "    subprocess.call(cmd_dswxs1.split()+dswxs1_warped_list)\n",
        "else:\n",
        "    print('path3')\n",
        "    print(\"No entries in dswxs1_warped_list. Something went wrong.\")"
      ],
      "metadata": {
        "id": "cAF65baA1a_L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ls m*"
      ],
      "metadata": {
        "id": "3U0gWeBfIkra"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 3: VFM"
      ],
      "metadata": {
        "id": "imRW8vGEQO6g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Initialize the AWS client. The credentials for this are blank because the AWS bucket we are pulling from is public."
      ],
      "metadata": {
        "id": "mJMyXUCzUs9U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cred = {\n",
        "    \"aws_access_key_id\": \"\",\n",
        "    \"aws_secret_access_key\": \"\",\n",
        "    \"config\": Config(signature_version = UNSIGNED)\n",
        "}\n",
        "\n",
        "session = boto3.Session()\n",
        "client = session.client('s3', **cred)"
      ],
      "metadata": {
        "id": "eUofAnqh6Szs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "VFM divides its tiles into 15 degree by 15 degree tiles"
      ],
      "metadata": {
        "id": "Naw7nSfH7HIc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def vfm_tile_reporter(lon, lat):\n",
        "  if lat > 75:\n",
        "    if lon > 90 and lon < 105:\n",
        "      vfm_tile = 50\n",
        "    if lon > 105 and lon < 120:\n",
        "      vfm_tile = 51\n",
        "  if lat > 60 and lat < 75:\n",
        "    if lon < -165:\n",
        "      vfm_tile = 1\n",
        "    if lon > -165 and lon < -150:\n",
        "      vfm_tile = 2\n",
        "    if lon > -150 and lon < -135:\n",
        "      vfm_tile = 3\n",
        "    if lon > -135 and lon < -120:\n",
        "      vfm_tile = 4\n",
        "    if lon > -120 and lon < -105:\n",
        "      vfm_tile = 5\n",
        "    if lon > -105 and lon < -90:\n",
        "      vfm_tile = 6\n",
        "    if lon > -90 and lon < -75:\n",
        "      vfm_tile = 7\n",
        "    if lon > -75 and lon < -60:\n",
        "      vfm_tile = 8\n",
        "    if lon > -60 and lon < -45:\n",
        "      vfm_tile = 130\n",
        "    if lon > -45 and lon < -30:\n",
        "      vfm_tile = 131\n",
        "    if lon > -30 and lon < -15:\n",
        "      vfm_tile = 132\n",
        "    if lon > -15 and lon < 0:\n",
        "      vfm_tile = 42\n",
        "    if lon > 0 and lon < 15:\n",
        "      vfm_tile = 43\n",
        "    if lon > 15 and lon < 30:\n",
        "      vfm_tile = 44\n",
        "    if lon > 30 and lon < 45:\n",
        "      vfm_tile = 45\n",
        "    if lon > 45 and lon < 60:\n",
        "      vfm_tile = 46\n",
        "    if lon > 60 and lon < 75:\n",
        "      vfm_tile = 47\n",
        "    if lon > 75 and lon < 90:\n",
        "      vfm_tile = 48\n",
        "    if lon > 90 and lon < 105:\n",
        "      vfm_tile = 49\n",
        "    if lon > 105 and lon < 120:\n",
        "      vfm_tile = 52\n",
        "    if lon > 120 and lon < 135:\n",
        "      vfm_tile = 53\n",
        "    if lon > 135 and lon < 150:\n",
        "      vfm_tile = 54\n",
        "    if lon > 150 and lon < 165:\n",
        "      vfm_tile = 55\n",
        "    if lon > 165 and lon < 180:\n",
        "      vfm_tile = 56\n",
        "  if lat > 45 and lat < 60:\n",
        "    if lon < -165:\n",
        "      vfm_tile = 9\n",
        "    if lon > -165 and lon < -150:\n",
        "      vfm_tile = 10\n",
        "    if lon > -150 and lon < -135:\n",
        "      vfm_tile = 129\n",
        "    if lon > -135 and lon < -120:\n",
        "      vfm_tile = 11\n",
        "    if lon > -120 and lon < -105:\n",
        "      vfm_tile = 12\n",
        "    if lon > -105 and lon < -90:\n",
        "      vfm_tile = 13\n",
        "    if lon > -90 and lon < -75:\n",
        "      vfm_tile = 14\n",
        "    if lon > -75 and lon < -60:\n",
        "      vfm_tile = 15\n",
        "    if lon > -60 and lon < -45:\n",
        "      vfm_tile = 16\n",
        "    if lon > -15 and lon < 0:\n",
        "      vfm_tile = 57\n",
        "    if lon > 0 and lon < 15:\n",
        "      vfm_tile = 58\n",
        "    if lon > 15 and lon < 30:\n",
        "      vfm_tile = 59\n",
        "    if lon > 30 and lon < 45:\n",
        "      vfm_tile = 60\n",
        "    if lon > 45 and lon < 60:\n",
        "      vfm_tile = 61\n",
        "    if lon > 60 and lon < 75:\n",
        "      vfm_tile = 62\n",
        "    if lon > 75 and lon < 90:\n",
        "      vfm_tile = 63\n",
        "    if lon > 90 and lon < 105:\n",
        "      vfm_tile = 64\n",
        "    if lon > 105 and lon < 120:\n",
        "      vfm_tile = 65\n",
        "    if lon > 120 and lon < 135:\n",
        "      vfm_tile = 66\n",
        "    if lon > 135 and lon < 150:\n",
        "      vfm_tile = 67\n",
        "    if lon > 150 and lon < 165:\n",
        "      vfm_tile = 68\n",
        "    if lon > 165 and lon < 180:\n",
        "      vfm_tile = 69\n",
        "  if lat > 30 and lat < 45:\n",
        "    if lon > -135 and lon < -120:\n",
        "      vfm_tile = 17\n",
        "    if lon > -120 and lon < -105:\n",
        "      vfm_tile = 18\n",
        "    if lon > -105 and lon < -90:\n",
        "      vfm_tile = 19\n",
        "    if lon > -90 and lon < -75:\n",
        "      vfm_tile = 20\n",
        "    if lon > -75 and lon < -60:\n",
        "      vfm_tile = 21\n",
        "    if lon > -15 and lon < 0:\n",
        "      vfm_tile = 70\n",
        "    if lon > 0 and lon < 15:\n",
        "      vfm_tile = 71\n",
        "    if lon > 15 and lon < 30:\n",
        "      vfm_tile = 72\n",
        "    if lon > 30 and lon < 45:\n",
        "      vfm_tile = 73\n",
        "    if lon > 45 and lon < 60:\n",
        "      vfm_tile = 74\n",
        "    if lon > 60 and lon < 75:\n",
        "      vfm_tile = 75\n",
        "    if lon > 75 and lon < 90:\n",
        "      vfm_tile = 76\n",
        "    if lon > 90 and lon < 105:\n",
        "      vfm_tile = 77\n",
        "    if lon > 105 and lon < 120:\n",
        "      vfm_tile = 78\n",
        "    if lon > 120 and lon < 135:\n",
        "      vfm_tile = 79\n",
        "    if lon > 135 and lon < 150:\n",
        "      vfm_tile = 80\n",
        "    if lon > 150 and lon < 165:\n",
        "      vfm_tile = 81\n",
        "  if lat > 15 and lat < 30:\n",
        "    if lon > -165 and lon < -150:\n",
        "      vfm_tile = 133\n",
        "    if lon > -120 and lon < -105:\n",
        "      vfm_tile = 22\n",
        "    if lon > -105 and lon < -90:\n",
        "      vfm_tile = 23\n",
        "    if lon > -90 and lon < -75:\n",
        "      vfm_tile = 24\n",
        "    if lon > -75 and lon < -60:\n",
        "      vfm_tile = 25\n",
        "    if lon > -30 and lon < -15:\n",
        "      vfm_tile = 82\n",
        "    if lon > -15 and lon < 0:\n",
        "      vfm_tile = 83\n",
        "    if lon > 0 and lon < 15:\n",
        "      vfm_tile = 84\n",
        "    if lon > 15 and lon < 30:\n",
        "      vfm_tile = 85\n",
        "    if lon > 30 and lon < 45:\n",
        "      vfm_tile = 86\n",
        "    if lon > 45 and lon < 60:\n",
        "      vfm_tile = 87\n",
        "    if lon > 60 and lon < 75:\n",
        "      vfm_tile = 88\n",
        "    if lon > 75 and lon < 90:\n",
        "      vfm_tile = 89\n",
        "    if lon > 90 and lon < 105:\n",
        "      vfm_tile = 90\n",
        "    if lon > 105 and lon < 120:\n",
        "      vfm_tile = 91\n",
        "    if lon > 120 and lon < 135:\n",
        "      vfm_tile = 92\n",
        "  if lat > 0 and lat < 15:\n",
        "    if lon > -105 and lon < -90:\n",
        "      vfm_tile = 26\n",
        "    if lon > -90 and lon < -75:\n",
        "      vfm_tile = 27\n",
        "    if lon > -75 and lon < -60:\n",
        "      vfm_tile = 28\n",
        "    if lon > -60 and lon < -45:\n",
        "      vfm_tile = 29\n",
        "    if lon > -30 and lon < -15:\n",
        "      vfm_tile = 93\n",
        "    if lon > -15 and lon < 0:\n",
        "      vfm_tile = 94\n",
        "    if lon > 0 and lon < 15:\n",
        "      vfm_tile = 95\n",
        "    if lon > 15 and lon < 30:\n",
        "      vfm_tile = 96\n",
        "    if lon > 30 and lon < 45:\n",
        "      vfm_tile = 97\n",
        "    if lon > 45 and lon < 60:\n",
        "      vfm_tile = 98\n",
        "    if lon > 60 and lon < 75:\n",
        "      vfm_tile = 99\n",
        "    if lon > 75 and lon < 90:\n",
        "      vfm_tile = 100\n",
        "    if lon > 90 and lon < 105:\n",
        "      vfm_tile = 101\n",
        "    if lon > 105 and lon < 120:\n",
        "      vfm_tile = 102\n",
        "    if lon > 120 and lon < 135:\n",
        "      vfm_tile = 103\n",
        "  if lat > -15 and lat < 0:\n",
        "    if lon < -165:\n",
        "      vfm_tile = 136\n",
        "    if lon > -90 and lon < -75:\n",
        "      vfm_tile = 30\n",
        "    if lon > -75 and lon < -60:\n",
        "      vfm_tile = 31\n",
        "    if lon > -60 and lon < -45:\n",
        "      vfm_tile = 32\n",
        "    if lon > -45 and lon < -30:\n",
        "      vfm_tile = 33\n",
        "    if lon > 0 and lon < 15:\n",
        "      vfm_tile = 104\n",
        "    if lon > 15 and lon < 30:\n",
        "      vfm_tile = 105\n",
        "    if lon > 30 and lon < 45:\n",
        "      vfm_tile = 106\n",
        "    if lon > 45 and lon < 60:\n",
        "      vfm_tile = 107\n",
        "    if lon > 75 and lon < 90:\n",
        "      vfm_tile = 108\n",
        "    if lon > 90 and lon < 105:\n",
        "      vfm_tile = 109\n",
        "    if lon > 105 and lon < 120:\n",
        "      vfm_tile = 110\n",
        "    if lon > 120 and lon < 135:\n",
        "      vfm_tile = 111\n",
        "    if lon > 135 and lon < 150:\n",
        "      vfm_tile = 112\n",
        "    if lon > 150 and lon < 165:\n",
        "      vfm_tile = 134\n",
        "  if lat > -30 and lat < -15:\n",
        "    if lon > -75 and lon < -60:\n",
        "      vfm_tile = 34\n",
        "    if lon > -60 and lon < -45:\n",
        "      vfm_tile = 35\n",
        "    if lon > -45 and lon < -30:\n",
        "      vfm_tile = 36\n",
        "    if lon > 0 and lon < 15:\n",
        "      vfm_tile = 113\n",
        "    if lon > 15 and lon < 30:\n",
        "      vfm_tile = 114\n",
        "    if lon > 30 and lon < 45:\n",
        "      vfm_tile = 115\n",
        "    if lon > 45 and lon < 60:\n",
        "      vfm_tile = 116\n",
        "    if lon > 105 and lon < 120:\n",
        "      vfm_tile = 117\n",
        "    if lon > 120 and lon < 135:\n",
        "      vfm_tile = 118\n",
        "    if lon > 135 and lon < 150:\n",
        "      vfm_tile = 119\n",
        "    if lon > 150 and lon < 165:\n",
        "      vfm_tile = 120\n",
        "    if lon > 165 and lon < 180:\n",
        "      vfm_tile = 135\n",
        "  if lat > -45 and lat < -30:\n",
        "    if lon > -75 and lon < -60:\n",
        "      vfm_tile = 37\n",
        "    if lon > -60 and lon < -45:\n",
        "      vfm_tile = 38\n",
        "    if lon > 15 and lon < 30:\n",
        "      vfm_tile = 121\n",
        "    if lon > 30 and lon < 45:\n",
        "      vfm_tile = 122\n",
        "    if lon > 105 and lon < 120:\n",
        "      vfm_tile = 123\n",
        "    if lon > 120 and lon < 135:\n",
        "      vfm_tile = 124\n",
        "    if lon > 135 and lon < 150:\n",
        "      vfm_tile = 125\n",
        "    if lon > 150 and lon < 165:\n",
        "      vfm_tile = 126\n",
        "    if lon > 165 and lon < 180:\n",
        "      vfm_tile = 127\n",
        "  if lat > -60 and lat < -45:\n",
        "    if lon > -90 and lon < -75:\n",
        "      vfm_tile = 39\n",
        "    if lon < -75 and lon < -60:\n",
        "      vfm_tile = 40\n",
        "    if lon < -60 and lon < -45:\n",
        "      vfm_tile = 41\n",
        "    if lon > 165:\n",
        "      vfm_tile = 128\n",
        "\n",
        "  return vfm_tile"
      ],
      "metadata": {
        "id": "E5e6A0PYU11E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vfm_tile_list = []"
      ],
      "metadata": {
        "id": "d7PCB84TfADq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tile1 = vfm_tile_reporter(lon_min, lat_min)\n",
        "if tile1 < 100:\n",
        "  tile1str = '0' + str(tile1)\n",
        "else:\n",
        "  tile1str = str(tile1)\n",
        "\n",
        "tile2 = vfm_tile_reporter(lon_min, lat_max)\n",
        "if tile1 < 100:\n",
        "  tile2str = '0' + str(tile2)\n",
        "else:\n",
        "  tile2str = str(tile2)\n",
        "\n",
        "tile3 = vfm_tile_reporter(lon_max, lat_min)\n",
        "if tile3 < 100:\n",
        "  tile3str = '0' + str(tile3)\n",
        "else:\n",
        "  tile3str = str(tile3)\n",
        "\n",
        "tile4 = vfm_tile_reporter(lon_max, lat_max)\n",
        "if tile4 < 100:\n",
        "  tile4str = '0' + str(tile4)\n",
        "else:\n",
        "  tile4str = str(tile4)"
      ],
      "metadata": {
        "id": "iFrybsd7Wu8-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vfm_tile_list.append(tile1str)\n",
        "if tile2str not in vfm_tile_list:\n",
        "  vfm_tile_list.append(tile2str)\n",
        "if tile3str not in vfm_tile_list:\n",
        "  vfm_tile_list.append(tile3str)\n",
        "if tile4str not in vfm_tile_list:\n",
        "  vfm_tile_list.append(tile4str)"
      ],
      "metadata": {
        "id": "_RX43V8eQRDI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "my_date_formatted = my_start_date.replace('-', '/')"
      ],
      "metadata": {
        "id": "3ION5BCIf4KL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pwd"
      ],
      "metadata": {
        "id": "csjYS2ESL-ml"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.chdir('..')\n",
        "os.mkdir('VFM')\n",
        "os.chdir('VFM')"
      ],
      "metadata": {
        "id": "ANArs0I0hsKS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ls"
      ],
      "metadata": {
        "id": "g0DQQFQDzZUY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vfm_paths_list = []\n",
        "gdrive_paths_list = []\n",
        "vfm_bucket = 'noaa-jpss'\n",
        "\n",
        "\n",
        "\n",
        "for j in range(len(vfm_tile_list)):\n",
        "  # Get tile number\n",
        "  vfm_tile = vfm_tile_list[j]\n",
        "\n",
        "  # Use paginator\n",
        "  paginator = client.get_paginator('list_objects_v2')\n",
        "  operation_parameters = {'Bucket': 'noaa-jpss',\n",
        "                        'Prefix': 'JPSS_Blended_Products/VFM_1day_GLB/TIF/' + my_date_formatted + '/VIIRS-Flood-1day-GLB' + vfm_tile}\n",
        "  page_iterator = paginator.paginate(**operation_parameters)\n",
        "  for page in page_iterator:\n",
        "    my_key = page['Contents'][0]['Key']\n",
        "    print(my_key)\n",
        "\n",
        "  my_gdrive_path = my_Gdrive_folder + 'VFM/vfm_pt' + str(j+1) + '.tif'\n",
        "  client.download_file(vfm_bucket, my_key, my_gdrive_path)"
      ],
      "metadata": {
        "id": "3jjBe3yOgAoS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ls"
      ],
      "metadata": {
        "id": "2yoVJk6h89Gq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vfm_list = glob.glob(\"vfm_pt[0-9].tif\")\n",
        "\n",
        "for h in range(len(vfm_list)):\n",
        "  vfm_img = vfm_list[int(h)]\n",
        "  vfm_outstring = 'vfm_reproj_pt' + str(h+1) + '.tif'\n",
        "  gdal.Warp(vfm_outstring, vfm_img, dstSRS = final_proj_string)"
      ],
      "metadata": {
        "id": "eyRGIcPH57WV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ls"
      ],
      "metadata": {
        "id": "YZc4TPsr0AGN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vfm_warped_list = glob.glob(\"vfm_reproj_pt[0-9].tif\")"
      ],
      "metadata": {
        "id": "-8prGJCy86a7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if len(vfm_warped_list) > 0:\n",
        "  if len(vfm_warped_list) == 1:\n",
        "    print('path2')\n",
        "    os.rename(vfm_warped_list[0], 'merged_vfm_' + flood_event_desc + '.tif')\n",
        "  else:\n",
        "    print('path1')\n",
        "    vfm_warped_list = glob.glob(\"vfm_reproj_pt[0-9].tif\")\n",
        "    cmd_vfm = \"gdal_merge.py -o merged_vfm_\" + flood_event_desc + \".tif\"\n",
        "    subprocess.call(cmd_vfm.split()+vfm_warped_list)\n",
        "else:\n",
        "    print('path3')\n",
        "    print(\"No entries in vfm_warped_list. Something went wrong.\")\n"
      ],
      "metadata": {
        "id": "zpWpv3oxPnIq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ls"
      ],
      "metadata": {
        "id": "745AlTS90aZQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Part 4: GFM"
      ],
      "metadata": {
        "id": "60wfAu7nM7FH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.chdir('..')\n",
        "os.mkdir('GFM')\n",
        "os.chdir('GFM')"
      ],
      "metadata": {
        "id": "oZ1Ylp_tLbor"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pwd"
      ],
      "metadata": {
        "id": "v6wIT9nsIBg7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 4 Step 1: Get client_id and access token"
      ],
      "metadata": {
        "id": "hG24Sj6CP_sT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "base_url = 'https://api.gfm.eodc.eu/v2' #base_url = 'https://api.gfm.eodc.eu/v1'\n",
        "\n",
        "data = {\n",
        "    \"email\": my_gfm_email,\n",
        "    \"password\": my_gfm_password\n",
        "}\n",
        "\n",
        "auth_url = f\"{base_url}/auth/login\"\n",
        "\n",
        "auth_response = requests.post(auth_url, json=data)\n",
        "\n",
        "client_id = auth_response.json()[\"client_id\"]\n",
        "access_token = auth_response.json()[\"access_token\"]"
      ],
      "metadata": {
        "id": "04ka4aR5Gr2n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 4 Step 2: Create AOI"
      ],
      "metadata": {
        "id": "V3V94YKESA5h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "aoi_data = {\n",
        "  \"aoi_name\": flood_event_desc,\n",
        "  \"description\": flood_event_desc,\n",
        "  \"user_id\": client_id,\n",
        "  \"geoJSON\": {\n",
        "    \"type\": \"Polygon\",\n",
        "    \"coordinates\": [\n",
        "      [\n",
        "        [\n",
        "          lon_min,\n",
        "          lat_min\n",
        "        ],\n",
        "        [\n",
        "          lon_max,\n",
        "          lat_min\n",
        "        ],\n",
        "        [\n",
        "          lon_max,\n",
        "          lat_max\n",
        "        ],\n",
        "        [\n",
        "          lon_min,\n",
        "          lat_max\n",
        "        ],\n",
        "        [\n",
        "          lon_min,\n",
        "          lat_min\n",
        "        ]\n",
        "      ]\n",
        "    ]\n",
        "  },\n",
        "  \"region\": \"AUT/Tirol\",\n",
        "  \"skip_aoi_check\": False\n",
        "}\n",
        "\n",
        "aoi_url = f\"{base_url}/aoi/create\"\n",
        "\n",
        "headers = {\"Authorization\": f\"bearer {access_token}\"}\n",
        "headers2 = {\"Authorization\": f\"Bearer {access_token}\"}\n",
        "aoi_request = requests.post(aoi_url, json=aoi_data, headers=headers)\n",
        "\n",
        "final_aoi_id = aoi_request.json()[\"aoi_id\"]"
      ],
      "metadata": {
        "id": "0I7-SylUHr_g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "aoi_request.status_code"
      ],
      "metadata": {
        "id": "GW_w1kvyM2Kt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_aoi_id"
      ],
      "metadata": {
        "id": "43PkulXAlMNo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 4 Step 3: Get Product ID's for the aoi_id we retrieved in step 2"
      ],
      "metadata": {
        "id": "ZacL2oGKlp2f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "my_start_date_gfm = my_start_date + 'T00:00:00'\n",
        "myday_after_gfm = myday_after + 'T00:00:00'"
      ],
      "metadata": {
        "id": "3GxCkglPmgPc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "product_url = f\"{base_url}/aoi/{final_aoi_id}/products\"\n",
        "\n",
        "product_params = {\n",
        "    \"time\": \"range\",\n",
        "    \"from\": my_start_date_gfm,\n",
        "    \"to\": myday_after_gfm\n",
        "}\n",
        "\n",
        "product_response = requests.get(product_url, headers=headers, params = product_params)\n",
        "\n",
        "if product_response.status_code == 200:\n",
        "    print(\"Available Products:\", product_response.json())\n",
        "else:\n",
        "  print(\"Error:\", product_response.status_code, product_response.text)"
      ],
      "metadata": {
        "id": "Qdq0oByDl9iv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "product_dict = product_response.json()\n",
        "gfm_products = product_dict['products']\n",
        "product_ids = []\n",
        "\n",
        "for j in range(len(gfm_products)):\n",
        "  product_of_interest = gfm_products[j]\n",
        "  prodid = product_of_interest['product_id']\n",
        "  product_ids.append(prodid)"
      ],
      "metadata": {
        "id": "WDTEKRGgtjJl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cell_codes = []\n",
        "for k in range(len(gfm_products)):\n",
        "  prodofint = gfm_products[k]\n",
        "  sceneid = prodofint['cell_code']\n",
        "  cell_codes.append(sceneid)"
      ],
      "metadata": {
        "id": "DWR4sw44KnSj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cell_codes"
      ],
      "metadata": {
        "id": "Ier6bdPfLAaG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 4 Step 4: Download product IDs"
      ],
      "metadata": {
        "id": "9noJEplAEzBx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "my_layer_id = '2'"
      ],
      "metadata": {
        "id": "SGyhoGAmE4Wq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def download(url: str, dest_folder: str, file_name):\n",
        "    if not os.path.exists(dest_folder):\n",
        "        os.makedirs(dest_folder)  # create folder if it does not exist\n",
        "\n",
        "    filename = file_name\n",
        "    file_path = os.path.join(dest_folder, filename)\n",
        "\n",
        "    r = requests.get(url, stream=True)\n",
        "    if r.ok:\n",
        "        print(\"saving to\", os.path.abspath(file_path))\n",
        "        with open(file_path, 'wb') as f:\n",
        "            for chunk in r.iter_content(chunk_size=1024 * 8):\n",
        "                if chunk:\n",
        "                    f.write(chunk)\n",
        "                    f.flush()\n",
        "                    os.fsync(f.fileno())\n",
        "    else:  # HTTP status code 4XX/5XX\n",
        "        print(\"Download failed: status code {}\\n{}\".format(r.status_code, r.text))"
      ],
      "metadata": {
        "id": "NQSaR-9y_sQ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Thank you and credit to user Ivan Vinogradov on stackoverflow for the piece of code above"
      ],
      "metadata": {
        "id": "b-s1rb4RFv8N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pwd"
      ],
      "metadata": {
        "id": "OeRwEbga_7Pj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ls"
      ],
      "metadata": {
        "id": "-b1capWw__R0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for r in range(len(cell_codes)):\n",
        "  my_cell_code = cell_codes[r]\n",
        "  #download_url = f\"{base_url}/download/product/{product_ids[j]}\"\n",
        "  download_url = f\"{base_url}/download/scene-file/{my_cell_code}/{final_aoi_id}/{my_layer_id}\"\n",
        "  download_response = requests.get(download_url, headers=headers)\n",
        "  url_to_download = download_response.json()['download_link']\n",
        "  zip_folder = 'gfm_zip_pt' + str(r + 1) + '.zip'\n",
        "  download(url_to_download, my_Gdrive_folder + 'GFM', zip_folder)\n",
        "  with zipfile.ZipFile(my_Gdrive_folder + 'GFM/' + zip_folder, 'r') as zip_ref:\n",
        "    zip_ref.extractall(my_Gdrive_folder + 'GFM')"
      ],
      "metadata": {
        "id": "Wk-8WdTB_yiJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 4 Step 5: Reproject and Merge"
      ],
      "metadata": {
        "id": "5EJszXCo21n-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we want to get the ENSEMBLE_FLOOD layers, and warp them to the projection we need"
      ],
      "metadata": {
        "id": "_r0OdM_7TpQO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pwd"
      ],
      "metadata": {
        "id": "t06R8LcQHAfm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.chdir('..')\n",
        "os.chdir('GFM')"
      ],
      "metadata": {
        "id": "81ZSx8fDHBaF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ls *ENSEMBLE*"
      ],
      "metadata": {
        "id": "aLIQcc0QKTpL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gfm_list = glob.glob('*ENSEMBLE_OBSWATER*.tif')\n",
        "\n",
        "for i in range(len(gfm_list)):\n",
        "  instring = gfm_list[i]\n",
        "  outstring = 'gfm_pt' + str(i + 1) + '.tif'\n",
        "  gdal.Warp(outstring, instring, dstSRS = final_proj_string)"
      ],
      "metadata": {
        "id": "MTQbZ_7EKXn0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ls"
      ],
      "metadata": {
        "id": "KaXP__0BHXGG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gfm_list = glob.glob(\"gfm_pt*.tif\")\n",
        "cmd_gfm = \"gdal_merge.py -o merged_gfm_\" + flood_event_desc + '.tif'\n",
        "subprocess.call(cmd_gfm.split()+gfm_list)"
      ],
      "metadata": {
        "id": "YrvZdIj_LXaE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ls m*"
      ],
      "metadata": {
        "id": "OU_bhlJMH4Mg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 5: MCDWD"
      ],
      "metadata": {
        "id": "mVCi2wdVMUtl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**IMPORTANT:** There is no way to access MCDWD files for  more than a week in the past."
      ],
      "metadata": {
        "id": "hZEs9n74PULB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 5 Step 1: Find the MODIS tiles corresponding to your area of interest"
      ],
      "metadata": {
        "id": "N7TIV6JFUJJ8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget --no-check-certificate https://modis-land.gsfc.nasa.gov/pdf/sn_bound_10deg.txt"
      ],
      "metadata": {
        "id": "HE5LBYWtY3-8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def modis_tile_reporter(lon, lat):\n",
        "\n",
        "  data = np.genfromtxt('sn_bound_10deg.txt',\n",
        "                     skip_header = 7,\n",
        "                     skip_footer = 3)\n",
        "\n",
        "  in_tile = False\n",
        "  i = 0\n",
        "  while(not in_tile):\n",
        "    in_tile = lat >= data[i, 4] and lat <= data[i, 5] and lon >= data[i, 2] and lon <= data[i, 3]\n",
        "    i += 1\n",
        "\n",
        "  vert = data[i-1, 0]\n",
        "  horiz = data[i-1, 1]\n",
        "  #print('Vertical Tile:', vert, 'Horizontal Tile:', horiz)\n",
        "  if vert < 10:\n",
        "    vertstring = '0' + str(int(vert))\n",
        "  else:\n",
        "    vertstring = str(int(vert))\n",
        "  if horiz < 10:\n",
        "    horizstring = '0' + str(int(horiz))\n",
        "  else:\n",
        "    horizstring = str(int(horiz))\n",
        "  final_string = 'h'+horizstring+'v'+vertstring\n",
        "  return final_string\n"
      ],
      "metadata": {
        "id": "OMN-v4c6U84L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Credit to the [this website](https://www.earthdatascience.org/tutorials/convert-modis-tile-to-lat-lon/), from which I adapted the code above."
      ],
      "metadata": {
        "id": "WsXnZUE8ZEx9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bounds = [[lon_min, lat_min], [lon_max, lat_min], [lon_max, lat_max], [lon_min, lat_max]]\n",
        "modtiles = []\n",
        "\n",
        "for j in range(len(bounds)):\n",
        "  coordpair = bounds[j]\n",
        "  lon = coordpair[0]\n",
        "  lat = coordpair[1]\n",
        "  modtile = modis_tile_reporter(lon, lat)\n",
        "  if modtile not in modtiles:\n",
        "    modtiles.append(modtile)"
      ],
      "metadata": {
        "id": "NaCY2FvGZPwJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "modtiles\n"
      ],
      "metadata": {
        "id": "z_7tXXLdZM0Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 5 Step 2: Find the Julian Day corresponding to your area of interest\n",
        "\n",
        "Use [this website](https://www-air.larc.nasa.gov/tools/jday.htm) to convert your date of interest to the Julian Day."
      ],
      "metadata": {
        "id": "CssssI-G72-E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Step 1: Email earthdata-support@nasa.gov, and put \"lance flood\" in the subject line. In the body of the email, include the h-v tile, and the julian date and year for which your flood event occurs. Request the HDF file of the MCDWD product for this region and time period of interest.\n",
        "- Step 2: Wait. NASA will email you the HDF files associated with the product for your date of interest.\n",
        "- Step 3: Open Google Drive, and navigate to the Google Drive folder shown below"
      ],
      "metadata": {
        "id": "SVoAJ118YUGh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.chdir('..')"
      ],
      "metadata": {
        "id": "_o64Ma9__JYn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pwd"
      ],
      "metadata": {
        "id": "hXAZxcQm_M8U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Step 4: Create a folder called \"MCDWD\"\n",
        "* Step 5: Navigate into the MCDWD file\n",
        "* Step 6: Upload the HDF file you downloaded in Step 2"
      ],
      "metadata": {
        "id": "E-H72lyI_PyE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.chdir('MCDWD')"
      ],
      "metadata": {
        "id": "p9Ec45hU_cwX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ls"
      ],
      "metadata": {
        "id": "LEYM87v5_p9L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mydir = my_Gdrive_folder + '/MCDWD'"
      ],
      "metadata": {
        "id": "Tn8GDAWg_Pi9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for filename in os.listdir(mydir):\n",
        "  file_info = gdal.Info(filename)         # Use the gdal.Open method\n",
        "  print(file_info)\n",
        "  break                                   # Break the for loop so we just get the first file"
      ],
      "metadata": {
        "id": "QxhaDq3G_xW9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "count = 0    # Set a count variable that we will use to name the output files\n",
        "\n",
        "for filename in os.listdir(mydir):   # For each HDF file in the directory\n",
        "  gdaldataset = gdal.Open(filename)                                # Open the HDF file\n",
        "  subdataset = gdaldataset.GetSubDatasets()[0][0]                  # Navigate to the Subdataset of interest. In this case, we want Subdataset 1, which corresponds to the 0th index, since we start counting at 0.\n",
        "  count_str = str(count + 1)                                       # Convert the counter to a string for filename purposes\n",
        "  output_filename = 'mcdwd_pt' + count_str + '.tif'                # Each output file will be called 'mcdwd_pt1.tif', 'mcdwd_pt2.tif', etc.\n",
        "  gdal.Translate(output_filename, subdataset)                      # use the gdal.Translate method to create tifs from subdataset_1 of each hdf file\n",
        "  count += 1"
      ],
      "metadata": {
        "id": "_5_chyFo_1uF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ls"
      ],
      "metadata": {
        "id": "0Mtj9QRAA22D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mcdwd_list = glob.glob(\"mcdwd_pt*.tif\")\n",
        "mcdwd_list"
      ],
      "metadata": {
        "id": "_cbWCc-vCF5I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for t in range(len(mcdwd_list)):\n",
        "  outfile = 'mcdwd_reproj_pt_' + str(t) + '.tif'\n",
        "  infile = mcdwd_list[t]\n",
        "  gdal.Warp(outfile, infile, dstSRS = final_proj_string)"
      ],
      "metadata": {
        "id": "ccWfbNysA30u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mcdwd_warped_list = glob.glob(\"mcdwd_reproj_pt*.tif\")\n",
        "\n",
        "if len(mcdwd_warped_list) > 0:\n",
        "  if len(mcdwd_warped_list) == 1:\n",
        "    print('path2')\n",
        "    os.rename(mcdwd_warped_list[0], 'merged_mcdwd_' + flood_event_desc + '.tif')\n",
        "  else:\n",
        "    print('path1')\n",
        "    #vfm_warped_list = glob.glob(\"_reproj_pt[0-9].tif\")\n",
        "    cmd_mcdwd = \"gdal_merge.py -o merged_mcdwd_\" + flood_event_desc + \".tif\"\n",
        "    subprocess.call(cmd_mcdwd.split()+mcdwd_warped_list)\n",
        "else:\n",
        "    print('path3')\n",
        "    print(\"No entries in dswxs1_warped_list. Something went wrong.\")"
      ],
      "metadata": {
        "id": "ISiIkWrtBCQh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ls"
      ],
      "metadata": {
        "id": "6PTfGI2RCr3U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Part 6: HydroSAR"
      ],
      "metadata": {
        "id": "4tVFFoKUPa50"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before we can run HYDROSAR, we need to find out the Sentinel-1 granule names for our flood event of interest. We can use NASA's Common Metadata Repository (CMR) API to do so."
      ],
      "metadata": {
        "id": "5a29LVPliOvE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 6 Step 1: Identify Sentinel-1 Granules for time period and region of interest"
      ],
      "metadata": {
        "id": "wGbOnin9X5PH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "granule_ids = []\n",
        "#import requests\n",
        "\n",
        "# Define the API endpoint\n",
        "cmr_url = \"https://cmr.earthdata.nasa.gov/search/granules.json\"\n",
        "\n",
        "# Define the search parameters\n",
        "params = {\n",
        "    #\"short_name\": \"SENTINEL-1A_SLC\",\n",
        "    \"bounding_box\": f\"{lon_min},{lat_min},{lon_max},{lat_max}\",\n",
        "    \"temporal\":  f\"{my_start_date}T00:00:00Z,{my_start_date}T23:59:59Z\",\n",
        "    \"provider\": \"ASF\",\n",
        "    \"page_size\": 2000,  # Number of results per request\n",
        "}\n",
        "\n",
        "# Make the GET request\n",
        "response = requests.get(cmr_url, params=params)\n",
        "\n",
        "# Check if the request was successful\n",
        "if response.status_code == 200:\n",
        "    data = response.json()\n",
        "    granules = data.get(\"feed\", {}).get(\"entry\", [])\n",
        "\n",
        "    if granules:\n",
        "        #print(\"Sentinel-1 Granule IDs for Chad on September 26, 2024:\")\n",
        "        for granule in granules:\n",
        "            gran_title = granule[\"title\"]\n",
        "            if \"-GRD_HD\" in gran_title:\n",
        "              granule_ids.append(gran_title[:-7])\n",
        "              #print(gran_title)\n",
        "            #print(granule[\"title\"])  # Print granule ID\n",
        "    else:\n",
        "        print(\"No granules found for the specified date and location.\")\n",
        "else:\n",
        "    print(\"Error:\", response.status_code, response.text)\n"
      ],
      "metadata": {
        "id": "zQoQ-0yENkww"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 6 Step 2: Conduct RTC processing and run HYDROSAR Algorithm on Sentinel-1 Granules"
      ],
      "metadata": {
        "id": "CkcFH31VYDej"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hyp3 = sdk.HyP3(username = my_earthdata_login, password = my_earthdata_password)"
      ],
      "metadata": {
        "id": "45G-psabo38P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.chdir('..')\n",
        "os.mkdir('HYDROSAR')\n",
        "os.chdir('HYDROSAR')"
      ],
      "metadata": {
        "id": "fVTcuhqdcCph"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we will iterate through each of the s1 granules we found and iterate through them. For each granule, we will conduct Radiometric Terrain Correction (RTC) before running the HYDROSAR algorithm on the image. This will take approximately 30 minutes for each sentinel-1 granule you have"
      ],
      "metadata": {
        "id": "RpCby3hui64_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "names = []\n",
        "for t in range(len(granule_ids)):\n",
        "  mygran = granule_ids[t]\n",
        "  print(mygran)\n",
        "  t2 = t+1\n",
        "\n",
        "  name_ = f\"granule_{t2}\"\n",
        "\n",
        "  job = hyp3.submit_rtc_job(\n",
        "    mygran, name=name_,\n",
        "    radiometry='gamma0', scale='power', resolution=30, dem_name='copernicus',\n",
        "    include_dem=True, include_rgb=True,\n",
        "    speckle_filter=True, dem_matching=False\n",
        "  )\n",
        "\n",
        "  job = hyp3.watch(job)\n",
        "\n",
        "  product_zip = job.download_files()[0]\n",
        "  shutil.unpack_archive(product_zip)\n",
        "  vv_raster = product_zip.parent / product_zip.stem / f'{product_zip.stem}_VV.tif'\n",
        "  vh_raster = product_zip.parent / product_zip.stem / f'{product_zip.stem}_VH.tif'\n",
        "\n",
        "  make_water_map(f\"water_extent_pt{t2}.tif\", vv_raster, vh_raster, tile_shape=(100, 100),\n",
        "               max_vv_threshold=-15.5, max_vh_threshold=-23., hand_threshold=15., hand_fraction=0.8)\n"
      ],
      "metadata": {
        "id": "x19QfOE-bckQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 6 Part 3: Mosaic HYDROSAR images into a single map"
      ],
      "metadata": {
        "id": "iPRRjNzqYKYW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hydrosar_list = glob.glob(\"water_extent_pt[0-9].tif\")\n",
        "\n",
        "for p in range(len(hydrosar_list)):\n",
        "  p2 = p + 1\n",
        "  hs_img = hydrosar_list[p]\n",
        "  gdal.Warp(f\"hydrosar_pt{p2}.tif\", hs_img, dstSRS = final_proj_string)\n",
        "\n",
        "hydrosar_list = glob.glob(\"hydrosar_pt[0-9].tif\")\n",
        "cmd_hydrosar = \"gdal_merge.py -o merged_hydrosar\" + flood_event_desc + '.tif'\n",
        "subprocess.call(cmd_hydrosar.split()+hydrosar_list)"
      ],
      "metadata": {
        "id": "AYno-TDTjizE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 7: HYDRAFloods"
      ],
      "metadata": {
        "id": "kqX0-QhDjTEs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "roi = ee.FeatureCollection(my_gee_folder + 'aoi').geometry()"
      ],
      "metadata": {
        "id": "OpHRjzAAle_0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "my_start_date"
      ],
      "metadata": {
        "id": "0PURThmCrVk4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "myday_after"
      ],
      "metadata": {
        "id": "YlG7izEpMN4R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sent1 = hf.Dataset(\n",
        "    region = roi,\n",
        "    start_time = my_start_date,\n",
        "    end_time = myday_after,\n",
        "    asset_id = 'COPERNICUS/S1_GRD'\n",
        ")"
      ],
      "metadata": {
        "id": "3XJSbiSWM_FR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sent1.n_images"
      ],
      "metadata": {
        "id": "iC8eBb3TMPDi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sent1.dates"
      ],
      "metadata": {
        "id": "GgHE5lH1Oe0F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sent1_med2 = sent1.collection.median()"
      ],
      "metadata": {
        "id": "c9HlAFOJMixu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "elv = ee.Image(\"JAXA/ALOS/AW3D30/V2_2\").select(\"AVE_DSM\")\n",
        "\n",
        "sent1_speckle = sent1.apply_func(hf.gamma_map)\n",
        "\n",
        "sent1_tc = sent1_speckle.apply_func(corrections.slope_correction, elevation=elv, buffer=30)"
      ],
      "metadata": {
        "id": "FOdXMrA4lpLY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sent1_tc.n_images"
      ],
      "metadata": {
        "id": "1NKzKYAsNpJx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sent1_med = sent1_tc.collection.median()"
      ],
      "metadata": {
        "id": "909d6lY-lqth"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 7 Step 1: Initial Threshold Determination\n",
        "\n",
        "Next, we will run the HYDRAFloods algorithm. But first, we would like to know what a typical backscatter value of water will be in this region. To do this, let's get the Joint Research Center's Water Occurrence dataset to find permanent water bodies, then sample the permanent water bodies to get an estimate of the average backscatter value in the VV polarization."
      ],
      "metadata": {
        "id": "i0Y9ZERNpSvf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "my_start_date"
      ],
      "metadata": {
        "id": "iOIxb2aBrHOx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "aoi = ee.FeatureCollection(my_gee_folder + 'aoi')\n",
        "\n",
        "final_geom = aoi.geometry()"
      ],
      "metadata": {
        "id": "hxYjAznB8Jz4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "my_month = int(my_start_date[5:7])\n",
        "my_month"
      ],
      "metadata": {
        "id": "iEoRmgpVrNb-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "jrc = ee.ImageCollection(\"JRC/GSW1_4/MonthlyRecurrence\")\n",
        "jrc_size= jrc.size().getInfo()"
      ],
      "metadata": {
        "id": "DsjezAd36HuC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for j in range(jrc_size):\n",
        "  jrc_img = ee.Image(jrc.toList(jrc_size).get(j))\n",
        "  jrc_month = jrc_img.get('month').getInfo()\n",
        "  if jrc_month == my_month:\n",
        "    monthly_image = jrc_img.select(['monthly_recurrence'])"
      ],
      "metadata": {
        "id": "8NNm2OLN6V8B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Map = geemap.Map(center = (0, 0), zoom = 4)\n",
        "\n",
        "Map.addLayer(monthly_image)\n",
        "Map.addLayer(final_geom)\n",
        "\n",
        "Map"
      ],
      "metadata": {
        "id": "XDckLnGZuG1p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_points = 1e3"
      ],
      "metadata": {
        "id": "ZNAHHeuzPD3c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "my_strat_sample = monthly_image.stratifiedSample(\n",
        "    numPoints = 0,\n",
        "    classValues = [75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100],\n",
        "    classPoints = [num_points, num_points, num_points, num_points, num_points,\n",
        "                   num_points, num_points, num_points, num_points, num_points,\n",
        "                   num_points, num_points, num_points, num_points, num_points,\n",
        "                   num_points, num_points, num_points, num_points, num_points,\n",
        "                   num_points, num_points, num_points, num_points, num_points, num_points],\n",
        "    region = aoi,\n",
        "    geometries = True,\n",
        "    scale = 30,\n",
        "    projection = final_proj_string\n",
        ")"
      ],
      "metadata": {
        "id": "tRvuPGBm8zgh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "s1_sampled = sent1_med.select(['VV']).sampleRegions(\n",
        "    collection=my_strat_sample,\n",
        "    properties=['water'],\n",
        "    projection=final_proj_string,\n",
        "    scale=30\n",
        ")"
      ],
      "metadata": {
        "id": "bVpQCSlh92fl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following cell will take a couple minutes to work."
      ],
      "metadata": {
        "id": "NtRsfLGnUXeT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "s1_size = s1_sampled.size().getInfo()\n",
        "listed = s1_sampled.toList(s1_size).getInfo()\n",
        "\n",
        "vv_values = []\n",
        "\n",
        "for j in range(s1_size):\n",
        "  myfeat = listed[j]['properties']['VV']\n",
        "  vv_values.append(myfeat)\n",
        "\n",
        "vv_mean = np.mean(vv_values)\n",
        "vv_mean"
      ],
      "metadata": {
        "id": "TmpsUweKQ7_Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vv_mean"
      ],
      "metadata": {
        "id": "VwoafANPXEnP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 7 Step 2: Run Edge Otsu Algorithm"
      ],
      "metadata": {
        "id": "NBWuODxeqXDP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "edge = hf.edge_otsu(\n",
        "    sent1_med, #Sent1_med\n",
        "    band = 'VV',\n",
        "    region = roi,\n",
        "    edge_buffer = 100,\n",
        "    initial_threshold = vv_mean,\n",
        "    thresh_no_data = -15,\n",
        "    scale = 30\n",
        ").clip(roi)"
      ],
      "metadata": {
        "id": "98Z3fs5hltIa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 7 Step 3: Export to Google Drive or Google Earth Engine"
      ],
      "metadata": {
        "id": "7oI8Vfjr2rui"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "my_Gdrive_folder"
      ],
      "metadata": {
        "id": "xLXRY_3yw8at"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def gdrive_exporter(img, scale_):\n",
        "  region = roi\n",
        "  geemap.ee_export_image_to_drive(\n",
        "      image = img,\n",
        "      description = 'hydrafloods_' + flood_event_desc,\n",
        "      region = region,\n",
        "      scale = scale_,\n",
        "      crs = final_proj_string,\n",
        "      maxPixels = 1e13\n",
        "  )"
      ],
      "metadata": {
        "id": "rhDf6QgYlza2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Define a function that exports an Image to a Google Earth Engine Asset.\n",
        "def gee_exporter(img, scale_):\n",
        "\n",
        "  #desc = 'Flood_Map_Export_'\n",
        "  region_ = roi#.geometry()\n",
        "  geemap.ee_export_image_to_asset(image = img,\n",
        "                                  assetId = my_gee_folder + 'hydrafloods_mosaic', #my_gee_folder + 'hydrafloods_mosaic,\n",
        "                                  description = 'hydrafloods_export' + flood_event_desc,\n",
        "                                  region = region_,\n",
        "                                  crs = final_proj_string,\n",
        "                                  scale = scale_,\n",
        "                                  maxPixels = 1e13)\n",
        "  return 0"
      ],
      "metadata": {
        "id": "TTLYTc-mxbEL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you want to run the harmonization in Google Earth Engine (i.e. if you want to run Module 3A instead of Module 3), uncomment the following cell."
      ],
      "metadata": {
        "id": "1kqiqOZ83AEb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#gee_exporter(edge, 30)"
      ],
      "metadata": {
        "id": "cWhZtlF4mHHC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you want to run the harmonization in Google Earth Engine, comment out the following cell (or just do not run it).\n",
        "\n",
        "If you get a \"FileNotFound Error\" below, it is likely because the Google Drive export has not finished. You can check on the status of the GEE export by going to the [GEE Code Editor](https://code.earthengine.google.com/) and clicking on the \"Tasks\" tab in the upper right. Once the task is complete, you can rerun the cell below."
      ],
      "metadata": {
        "id": "0oGIKy2f37KC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gdrive_exporter(edge, 30)\n",
        "\n",
        "time.sleep(10 * 60)    # Comment this line out if you have already waited for the export to finish running.\n",
        "source_path = '/content/drive/MyDrive/hydrafloods_'+ flood_event_desc+'.tif'\n",
        "dest_path = '/content/' + my_Gdrive_folder + 'hydrafloods_' + flood_event_desc + '.tif'\n",
        "\n",
        "shutil.move(source_path, dest_path)"
      ],
      "metadata": {
        "id": "kqaRCV4px875"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cd .."
      ],
      "metadata": {
        "id": "wt0zRfRFyl3F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ls"
      ],
      "metadata": {
        "id": "t6OJb-OK0Zq_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 8: Upload Flood Maps to GEE (USER INPUT REQUIRED)\n",
        "\n",
        "This step is only necessary if you intend to do the harmonization in Google Earth Engine (i.e. running module 3A instead of 3)."
      ],
      "metadata": {
        "id": "VhOFDVI1ldIr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For each flood map (except for HYDRAFloods), do the following steps:\n",
        "\n",
        "1. Navigate to your Google Drive Folder defined at the beginning of this notebook\n",
        "2. Navigate to the subfolder corresponding to the product of interest\n",
        "3. Download the product named \"merged_..._.tif\"\n",
        "4. Go to the Google Earth Engine code editor\n",
        "5. Click \"Assets\" in the top corner\n",
        "6. Click \"New\"\n",
        "7. Click \"GEOTIFF\"\n",
        "8. Under the text that says \"Asset ID\", type in the path to your Google Earth Engine folder you defined at the beginning of this notebook. Then, type in the name of the flood product you are uploading followed by \"_mosaic\".\n",
        "  * Example: If I am uploading the hydrosar product and my GEE parent folder is \"users/mickymags/flood_intercomparison\", under the asset id text I would type in \"users/mickymags/flood_intercomparison/hydrosar_mosaic\"\n",
        "9. Click upload\n",
        "10. Repeat for the other 5 flood products"
      ],
      "metadata": {
        "id": "gIl8Yx0opqBa"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}